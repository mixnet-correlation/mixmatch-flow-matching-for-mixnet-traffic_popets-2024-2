{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8867ad-f855-4172-8176-214d4ec47da3",
   "metadata": {},
   "source": [
    "## Produce Datasets in Our Format, DeepCoFFEA Format, and MixCorr Format from Raw `exp06` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c82bc5-e4c9-49f9-ada3-cd07c646ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from os import makedirs\n",
    "from shutil import rmtree\n",
    "from datetime import datetime\n",
    "from os.path import join\n",
    "\n",
    "from lib_analyze_exps import parse_run, walk_one_level, create_mixcorr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8412a99-c801-43c6-ba8b-28ab60145bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY BEGIN ###\n",
    "\n",
    "# Specify the name of the experiment that is supposed to be processed from raw.\n",
    "EXP_NAME = \"exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay\"\n",
    "\n",
    "# Specify parent directory of raw experimental results directory.\n",
    "RAW_EXP_DIR_PARENT = \"/PRIVATE_PATH/\"\n",
    "\n",
    "###  MODIFY END  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c506f72-2f98-49e8-b315-722522a1fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct paths to:\n",
    "#   * RAW_EXP_DIR => raw experimental result files repository,\n",
    "#   * PROC_EXP_DIR_OUR => processed version of the dataset in our flowpair format after running this notebook,\n",
    "#   * PROC_EXP_DIR_DCOFFEA => processed version of the dataset in DeepCoFFEA flowpair format after running this notebook,\n",
    "#   * PROC_EXP_DIR_MIXCORR => processed version of the dataset in MixCorr flowpair format after running this notebook.\n",
    "#\n",
    "# CAUTION: The filesystem locations PROC_EXP_DIR_* will be deleted if they already exist.\n",
    "RAW_EXP_DIR = join(RAW_EXP_DIR_PARENT, f\"dataset_{EXP_NAME}_raw\")\n",
    "PROC_EXP_DIR_OUR = join(RAW_EXP_DIR_PARENT, f\"dataset_{EXP_NAME}_filtered-to-start-end\")\n",
    "PROC_EXP_DIR_DCOFFEA = join(RAW_EXP_DIR_PARENT, f\"dataset_{EXP_NAME}_format-deepcoffea_filtered-to-start-end\")\n",
    "PROC_EXP_DIR_MIXCORR = join(RAW_EXP_DIR_PARENT, f\"dataset_{EXP_NAME}_format-mixcorr_filtered-to-start-end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0cc64-8868-45b3-809d-578788d502b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete and recreate the final dataset directories.\n",
    "\n",
    "rmtree(PROC_EXP_DIR_OUR, ignore_errors = True)\n",
    "rmtree(PROC_EXP_DIR_DCOFFEA, ignore_errors = True)\n",
    "rmtree(PROC_EXP_DIR_MIXCORR, ignore_errors = True)\n",
    "\n",
    "makedirs(PROC_EXP_DIR_OUR, mode=0o755)\n",
    "makedirs(PROC_EXP_DIR_DCOFFEA, mode=0o755)\n",
    "makedirs(PROC_EXP_DIR_MIXCORR, mode=0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd21b36-78da-4d5f-99de-9b2e91720ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all succeeded runs to process.\n",
    "runs_succeeded = sorted(glob(join(RAW_EXP_DIR, \"*\", \"exp06_curl_run_*\", \"SUCCEEDED\")))\n",
    "\n",
    "# For exp06, this should give: 35,801.\n",
    "print(f\"{len(runs_succeeded):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452a755-b850-4647-8e8c-8e0748893e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special case for exp06: We had to 'split' the gateway-mixnodes log files as they were\n",
    "# too big to push to GitHub. Thus, before we run the production steps below, we reverse\n",
    "# the split step by concatenating each two pieces into one unified log file again.\n",
    "\n",
    "idx = 1\n",
    "exp_folders = walk_one_level(RAW_EXP_DIR, True)\n",
    "\n",
    "print(\"Unifying split gateway-mixnodes log files into a single log file again...\\n\")\n",
    "\n",
    "for exp_folder in exp_folders:\n",
    "    \n",
    "    gw_mix_log_one = join(exp_folder, \"logs_docker-run_gateway-mixnodes.log.aa\")\n",
    "    gw_mix_log_two = join(exp_folder, \"logs_docker-run_gateway-mixnodes.log.ab\")\n",
    "    gw_mix_log = join(exp_folder, \"logs_docker-run_gateway-mixnodes.log\")\n",
    "    \n",
    "    # Concatenate log one and log two into original log file.\n",
    "    ! cat {gw_mix_log_one} {gw_mix_log_two} > {gw_mix_log}\n",
    "    \n",
    "    # Count each file's number of lines.\n",
    "    lines_one = ! wc -l {gw_mix_log_one} | tr -s \" \" | cut -d \" \" -f 1\n",
    "    lines_two = ! wc -l {gw_mix_log_two} | tr -s \" \" | cut -d \" \" -f 1\n",
    "    lines_merged = ! wc -l {gw_mix_log} | tr -s \" \" | cut -d \" \" -f 1\n",
    "    \n",
    "    assert (int(lines_one[0]) + int(lines_two[0])) == int(lines_merged[0])\n",
    "    \n",
    "    # Extract first 10 lines from log one and the merged log to ensure correct order.\n",
    "    first_ten_lines_one = ! head -n 10 {gw_mix_log_one}\n",
    "    first_ten_lines_merged = ! head -n 10 {gw_mix_log}\n",
    "    \n",
    "    assert np.array_equal(first_ten_lines_one, first_ten_lines_merged) == True\n",
    "    \n",
    "    print(f\"\\t{idx}) Successfully merged split gateway-mixnodes log files into:\\n\\t{gw_mix_log}\\n\")\n",
    "    \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17367afe-5ba7-47c0-a569-d6ba43550ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Beginning to process raw exp06 dataset -----\\n\")\n",
    "\n",
    "run_ctr = 1\n",
    "\n",
    "for run_succeeded in runs_succeeded:\n",
    "    \n",
    "    parse_run(\n",
    "        PROC_EXP_DIR_OUR,\n",
    "        PROC_EXP_DIR_DCOFFEA,\n",
    "        run_ctr,\n",
    "        run_succeeded)\n",
    "    \n",
    "    run_ctr += 1\n",
    "\n",
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Finished processing raw exp06 dataset -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264dcab9-b158-4ec5-90c7-1a901d00cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily keep these lines until we run the entire Notebook again.\n",
    "rmtree(PROC_EXP_DIR_MIXCORR, ignore_errors = True)\n",
    "makedirs(PROC_EXP_DIR_MIXCORR, mode=0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04168b9c-6f6d-4592-8736-9942181a41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- [2023-02-23_16:50:39.129113] Beginning to produce MixCorr-formatted dataset for exp06 -----\n",
      "\n",
      "Considering flowpairs of dataset split 'train' now...\n",
      "Done with flowpairs of dataset split 'train'\n",
      "\n",
      "Considering flowpairs of dataset split 'val' now...\n",
      "Done with flowpairs of dataset split 'val'\n",
      "\n",
      "Considering flowpairs of dataset split 'test' now...\n",
      "Done with flowpairs of dataset split 'test'\n",
      "\n",
      "----- [2023-02-23_17:16:42.837233] Finished producing MixCorr-formatted dataset for exp06 -----\n"
     ]
    }
   ],
   "source": [
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Beginning to produce MixCorr-formatted dataset for exp06 -----\\n\")\n",
    "\n",
    "# Create dataset in MixCorr format based on existing dataset in our format.\n",
    "create_mixcorr_dataset(PROC_EXP_DIR_OUR, PROC_EXP_DIR_MIXCORR)\n",
    "\n",
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Finished producing MixCorr-formatted dataset for exp06 -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77d2a0a-6903-432c-b281-95e563347372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_ack.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_ack.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_ack.03'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_ack.04'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_ack.05'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_to_gateway_data.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_to_gateway_data.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_to_gateway_data.03'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_to_gateway_data.04'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_to_gateway_data.05'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_to_gateway_data.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_to_gateway_data.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_to_gateway_data.03'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_to_gateway_data.04'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_to_gateway_data.05'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_data.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_data.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_data.03'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_data.04'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_initiator_from_gateway_data.05'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/test_initiator_from_gateway_data.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/test_initiator_from_gateway_data.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/val_initiator_from_gateway_data.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/val_initiator_from_gateway_data.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_from_gateway_ack.01'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_from_gateway_ack.02'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_from_gateway_ack.03'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_from_gateway_ack.04'\n",
      "creating file '/PRIVATE_PATH/dataset_exp06_nym-binaries-1.0.2_static-http-download_longer-mix-delay_format-mixcorr_filtered-to-start-end/train_responder_from_gateway_ack.05'\n"
     ]
    }
   ],
   "source": [
    "# Split files in newly produced MixCorr-formatted dataset of at least 90 MiB into\n",
    "# multiple files in order to be able to commit to GitHub.\n",
    "\n",
    "files_too_large = ! find {PROC_EXP_DIR_MIXCORR} -type f -size +90M\n",
    "\n",
    "for file_too_large in files_too_large:\n",
    "    \n",
    "    ! split --verbose --lines=5000 --numeric-suffixes=01 {file_too_large} {file_too_large}.\n",
    "    ! cat {file_too_large}.?? > {PROC_EXP_DIR_MIXCORR}/tmp_file_too_large\n",
    "    ! cmp --verbose --print-bytes {PROC_EXP_DIR_MIXCORR}/tmp_file_too_large {file_too_large}\n",
    "    ! rm {file_too_large} {PROC_EXP_DIR_MIXCORR}/tmp_file_too_large"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
