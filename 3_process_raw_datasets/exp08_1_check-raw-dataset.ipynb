{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41388bd-945d-4ecd-b1da-3fa8dfe46f7c",
   "metadata": {},
   "source": [
    "## Run Assertion Checks on Raw `exp08` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0364629b-2100-4e0a-9afb-ad17e006df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from os.path import join, dirname, basename, getsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22e1098-a550-4e92-abeb-08c9994e9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY BEGIN ###\n",
    "\n",
    "# Specify the name of the experiment to be checked.\n",
    "EXP_NAME = \"exp08_nym-binaries-v1.1.13_static-http-download\"\n",
    "\n",
    "# Specify path to raw experimental result files.\n",
    "RAW_EXP_DIR_PARENT = \"/PRIVATE_PATH/\"\n",
    "\n",
    "###  MODIFY END  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e08716-400e-451d-b425-8e134519c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct path to RAW_EXP_DIR: raw experimental result files repository.\n",
    "RAW_EXP_DIR = join(RAW_EXP_DIR_PARENT, f\"dataset_{EXP_NAME}_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc6c40d-9fa1-4bb7-a2c3-926f3c9ba151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_gw_script(file: str):\n",
    "    \n",
    "    print(f\"-> Checking the gateway's experiment-running script '{basename(file)}'...\")\n",
    "    \n",
    "    print(\"\\n->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\")\n",
    "    script_correct_exp_id = ! grep -c \"^mixcorr_exp_id=\\\"exp08\\\"$\" {file}\n",
    "    print(f\"     {script_correct_exp_id[0]=}\")\n",
    "    assert int(script_correct_exp_id[0]) == 1, f\"{int(script_correct_exp_id[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\")\n",
    "    script_correct_nym_version = ! grep -c \"^mixcorr_nym_version=\\\"nym-binaries-v1.1.13\\\"$\" {file}\n",
    "    print(f\"     {script_correct_nym_version[0]=}\")\n",
    "    assert int(script_correct_nym_version[0]) == 1, f\"{int(script_correct_nym_version[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\")\n",
    "    script_correct_exp_name = ! grep -c \"^mixcorr_exp_name=\\\"\\${{mixcorr_exp_id}}_\\${{mixcorr_nym_version}}_static-http-download\\\"$\" {file}\n",
    "    print(f\"     {script_correct_exp_name[0]=}\")\n",
    "    assert int(script_correct_exp_name[0]) == 1, f\"{int(script_correct_exp_name[0])=} != 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1863f0ed-0935-4a4f-a1b1-482c48138302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gw_log(file: str):\n",
    "    \n",
    "    print(f\"\\n\\n-> Checking the gateway's experiment-running log file '{basename(file)}'...\")\n",
    "\n",
    "    print(\"\\n->-> Check that orchestration repository is checked out at the expected tag...\")\n",
    "    correct_orch_tag = ! grep -A 2 \"Logging git status of /root/mixcorr/data-collection_live-nym-network_hetzner...\" {file}\n",
    "    assert \"HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download\" in correct_orch_tag[1], \\\n",
    "        f\"'HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download' not in {correct_orch_tag[1]=}\"\n",
    "    assert \"nothing to commit, working tree clean\" in correct_orch_tag[2], \\\n",
    "        f\"'nothing to commit, working tree clean' not in {correct_orch_tag[2]=}\"\n",
    "    \n",
    "    print(\"\\n->-> Check that experiment scenarios repository is checked out at the expected tag...\")\n",
    "    correct_scens_tag = ! grep -A 2 \"Logging git status of /root/mixcorr/data-collection_experiments_nym...\" {file}\n",
    "    assert \"HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download\" in correct_scens_tag[1], \\\n",
    "        f\"'HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download' not in {correct_scens_tag[1]=}\"\n",
    "    assert \"nothing to commit, working tree clean\" in correct_scens_tag[2], \\\n",
    "        f\"'nothing to commit, working tree clean' not in {correct_scens_tag[2]=}\"\n",
    "    \n",
    "    print(\"\\n->-> Check that no log line contains the message that stored messages were pushed to an endpoint...\")\n",
    "    pushed_stored_msgs_not_present = ! grep \"\\[MIXCORR\\] \\[push_stored_messages_to_client\\] WARNING At least one on-disk message stored for endpoint \" {file}\n",
    "    print(f\"{pushed_stored_msgs_not_present=}\")\n",
    "    assert len(pushed_stored_msgs_not_present) == 0, f\"{len(pushed_stored_msgs_not_present)=} != 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac90c880-4f2b-4e2f-a8a6-3c3b9da9c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gw_sphinxflow_logs(flows_dir: str):\n",
    "    \n",
    "    print(f\"\\n\\n-> Checking SphinxFlow logs in '{flows_dir}'...\")\n",
    "    \n",
    "    print(\"\\n->-> Check that timestamps are monotonically increasing per SphinxFlow log file and observed message sizes are restricted to be in { 53, 1675, 2413 }...\")\n",
    "    \n",
    "    flow_files = sorted(glob(join(flows_dir, \"*.sphinxflow\")))\n",
    "    expected_msg_sizes = [53, 1675, 2413]\n",
    "    \n",
    "    for flow_file in flow_files:\n",
    "        \n",
    "        # Skip over any SphinxFlow files which contain only the header line\n",
    "        # 'msg_timestamp_nanos,msg_size_bytes\\n' (i.e., empty).\n",
    "        if getsize(flow_file) <= 35:\n",
    "            continue\n",
    "        \n",
    "        flow = np.loadtxt(fname = flow_file, dtype = (np.uint64, np.uint64), delimiter = \",\", skiprows = 1)\n",
    "        \n",
    "        ts_monotonically_increasing = np.all(np.diff(flow[:, 0]) > 0)\n",
    "        assert ts_monotonically_increasing == True, f\"{ts_monotonically_increasing=} != True\"\n",
    "        \n",
    "        sizes_expected = np.all(np.isin(flow[:, 1], expected_msg_sizes))\n",
    "        assert sizes_expected == True, f\"{sizes_expected=} != True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51572569-c8d0-4053-967b-f29261351737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raw_gateway_results(path: str):\n",
    "    \n",
    "    # Define all paths relevant to gateway results.\n",
    "    script_gw = join(path, \"4-exp08-run-experiments-nym-gateway.sh\")\n",
    "    log_gw = join(path, \"logs_4-exp08-run-experiments-nym-gateway.log\")\n",
    "    sphinxflow_logs = join(path, \"gateway_sphinxflows\")\n",
    "    \n",
    "    # Apply all checks.    \n",
    "    check_gw_script(script_gw)\n",
    "    check_gw_log(log_gw)\n",
    "    check_gw_sphinxflow_logs(sphinxflow_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61caacfb-f5b2-44f9-a755-cfcf083f05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_endp_script(file: str):\n",
    "    \n",
    "    print(f\"-> Checking experiment-running script of one endpoints instance '{basename(file)}'...\")\n",
    "    \n",
    "    print(\"\\n->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\")\n",
    "    script_correct_exp_id = ! grep -c \"^mixcorr_exp_id=\\\"exp08\\\"$\" {file}\n",
    "    print(f\"     {script_correct_exp_id[0]=}\")\n",
    "    assert int(script_correct_exp_id[0]) == 1, f\"{int(script_correct_exp_id[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\")\n",
    "    script_correct_nym_version = ! grep -c \"^mixcorr_nym_version=\\\"nym-binaries-v1.1.13\\\"$\" {file}\n",
    "    print(f\"     {script_correct_nym_version[0]=}\")\n",
    "    assert int(script_correct_nym_version[0]) == 1, f\"{int(script_correct_nym_version[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\")\n",
    "    script_correct_exp_name = ! grep -c \"^mixcorr_exp_name=\\\"\\${{mixcorr_exp_id}}_\\${{mixcorr_nym_version}}_static-http-download\\\"$\" {file}\n",
    "    print(f\"     {script_correct_exp_name[0]=}\")\n",
    "    assert int(script_correct_exp_name[0]) == 1, f\"{int(script_correct_exp_name[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\")\n",
    "    script_correct_file_size = ! grep -c \"^mixcorr_file_http_chars_num=1048576$\" {file}\n",
    "    print(f\"     {script_correct_file_size[0]=}\")\n",
    "    assert int(script_correct_file_size[0]) == 1, f\"{int(script_correct_file_size[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\")\n",
    "    script_correct_gw_data = ! grep -Pczo 'export mixcorr_static_private_gateway_owner=\"n16mgskx7xejdgpy9j8wxe944ech4zenrwjvm056\"\\nexport mixcorr_static_private_gateway_stake=\"100000000\"\\nexport mixcorr_static_private_gateway_location=\"Milky Way, Universe\"\\nexport mixcorr_static_private_gateway_host=\"159.69.196.183\"\\nexport mixcorr_static_private_gateway_mix_host=\"159.69.196.183:1789\"\\nexport mixcorr_static_private_gateway_clients_port=\"9000\"\\nexport mixcorr_static_private_gateway_version=\"1.1.13\"' {file}\n",
    "    print(f\"     {script_correct_gw_data[0]=}\")\n",
    "    assert int(script_correct_gw_data[0]) == 1, f\"{int(script_correct_gw_data[0])=} != 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddbdcfc-224e-4200-bbfa-234c729645bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_endp_log(results_dir: str, file: str):\n",
    "    \n",
    "    print(f\"\\n\\n-> Checking experiment log file '{basename(file)}'...\")\n",
    "    \n",
    "    print(\"\\n->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\")\n",
    "    log_correct_gw_identity_key = ! grep -c \"\\- mixcorr_static_private_gateway_identity_key='9cXBAoeGj7sWmMDk39XV3mP5GFY7JQv9LQ1wUrXc1B7u'$\" {file}\n",
    "    print(f\"     {log_correct_gw_identity_key[0]=}\")\n",
    "    assert int(log_correct_gw_identity_key[0]) == 1, f\"{int(log_correct_gw_identity_key[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\")\n",
    "    log_correct_gw_sphinx_key = ! grep -c \"\\- mixcorr_static_private_gateway_sphinx_key='7QhSZxbLY6ryrzsWTNAMEawLPuhpT5P1Tk8i6aoJNn1g'$\" {file}\n",
    "    print(f\"     {log_correct_gw_sphinx_key[0]=}\")\n",
    "    assert int(log_correct_gw_sphinx_key[0]) == 1, f\"{int(log_correct_gw_sphinx_key[0])=} != 1\"\n",
    "    \n",
    "    print(\"\\n->-> Check that orchestration repository is checked out at the expected tag...\")\n",
    "    log_correct_orch_tag = ! grep -A 2 \"Logging git status of /root/mixcorr/data-collection_live-nym-network_hetzner...\" {file}\n",
    "    assert \"HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download\" in log_correct_orch_tag[1], \\\n",
    "        f\"'HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download' not in {log_correct_orch_tag[1]=}\"\n",
    "    assert \"nothing to commit, working tree clean\" in log_correct_orch_tag[2], \\\n",
    "        f\"'nothing to commit, working tree clean' not in {log_correct_orch_tag[2]=}\"\n",
    "    \n",
    "    print(\"\\n->-> Check that experiment scenarios repository is checked out at the expected tag...\")\n",
    "    log_correct_scens_tag = ! grep -A 2 \"Logging git status of /root/mixcorr/data-collection_experiments_nym...\" {file}\n",
    "    assert \"HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download\" in log_correct_scens_tag[1], \\\n",
    "        f\"'HEAD detached at VERSION_FOR_exp08_nym-binaries-v1.1.13_static-http-download' not in {log_correct_scens_tag[1]=}\"\n",
    "    assert \"nothing to commit, working tree clean\" in log_correct_scens_tag[2], \\\n",
    "        f\"'nothing to commit, working tree clean' not in {log_correct_scens_tag[2]=}\"\n",
    "    \n",
    "    print(\"\\n->-> Checking that random file of correct size has been created...\")\n",
    "    exp_file_generated = ! grep -A 1 \"Generating 1048566 random characters for experiment document of size 1048576 Bytes (10 remaining characters will be run-specific ID)...\" {file}\n",
    "    exp_file_generated = \"\\n\".join(exp_file_generated)\n",
    "    assert \"-rw-r--r-- 1 root root 1048566 \" in exp_file_generated, f\"Unexpected permissions and/or size or generated file: '{exp_file_generated}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7acb13b6-7412-4b6e-a395-0a99b4f4babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_runs(results_dir: str):\n",
    "    \n",
    "    print(\"\\n\\n-> Checking result folders of all runs in this experiment...\")\n",
    "    \n",
    "    # Find all succeeded and failed runs.\n",
    "    runs_dirs = sorted(glob(join(results_dir, \"exp08_curl_run_*\")))\n",
    "    runs_succeeded_files = sorted(glob(join(results_dir, \"exp08_curl_run_*\", \"SUCCEEDED\")))\n",
    "    runs_failed_files = sorted(glob(join(results_dir, \"exp08_curl_run_*\", \"FAILED\")))\n",
    "    runs_completed_files = [ *runs_succeeded_files, *runs_failed_files ]\n",
    "    \n",
    "    assert len(runs_completed_files) == (len(runs_succeeded_files) + len(runs_failed_files)), \\\n",
    "        f\"{len(runs_completed_files)=} != ({len(runs_succeeded_files)=} + {len(runs_failed_files)=})\"\n",
    "    \n",
    "    print(f\"\\n->-> Of a total of {len(runs_dirs)=} runs, {len(runs_succeeded_files)=} SUCCEEDED and {len(runs_failed_files)=} FAILED...\")\n",
    "    \n",
    "    # Convert lists with run indicators (SUCCEEDED or FAILED files) into lists containing\n",
    "    # the files respective parent directories, i.e., the individual run folders.\n",
    "    runs_succeeded = sorted(list(map(lambda file: dirname(file), runs_succeeded_files)))\n",
    "    runs_failed = sorted(list(map(lambda file: dirname(file), runs_failed_files)))\n",
    "    runs_completed = sorted(list(map(lambda file: dirname(file), runs_completed_files)))\n",
    "    \n",
    "    # Glob for nym-network-requester configuration file in each SUCCEEDED run folder.\n",
    "    networkreq_confs = list(map(lambda run_dir: \n",
    "                            glob(join(run_dir, \"nym_folder\", \"service-providers\", \"network-requester\", \"network_requester_*\", \"config\", \"config.toml\"))[0],\n",
    "                            runs_succeeded))\n",
    "    \n",
    "    # Glob for nym-socks5-client configuration file in each SUCCEEDED run folder.\n",
    "    socks5client_confs = list(map(lambda run_dir:\n",
    "                                  glob(join(run_dir, \"nym_folder\", \"socks5-clients\", \"socks5-client_*\", \"config\", \"config.toml\"))[0],\n",
    "                                  runs_succeeded))\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\")\n",
    "    runs_completed_unique = set(runs_completed)\n",
    "    print(f\"{len(runs_completed)=}\")\n",
    "    print(f\"{len(runs_completed_unique)=}\")\n",
    "    assert len(runs_completed) == len(runs_completed_unique), f\"{len(runs_completed)=} != {len(runs_completed_unique)=}\"\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \\\"Proxy (started|finished)\\\" log lines)...\")\n",
    "    for run_succeeded_file in runs_succeeded_files:\n",
    "        \n",
    "        # Obtain the two respective log files (socks5-client, requester-server) of this SUCCEEDED run.\n",
    "        log_socks5_client = glob(join(dirname(run_succeeded_file), \"logs_socks5-client_run-*.log\"))[0]\n",
    "        log_requester_server = glob(join(dirname(run_succeeded_file), \"logs_requester-server_run-*.log\"))[0]\n",
    "        \n",
    "        # Ensure that the log line is present in the socks5-client log that the proxied HTTP request was started.\n",
    "        log_socks5_client_proxy_start = ! grep -c \"> Starting proxy for 127.0.0.1:9909 (id: \" {log_socks5_client}\n",
    "        assert int(log_socks5_client_proxy_start[0]) == 1, f\"{int(log_socks5_client_proxy_start[0])=} != 1\"\n",
    "        \n",
    "        # Ensure that the log line is present in the network-requester log that the proxied HTTP request was started.\n",
    "        log_requester_server_proxy_start = ! grep -c \"> Starting proxy for 127.0.0.1:9909 (currently there are 1 proxies being handled)$\" {log_requester_server}\n",
    "        assert int(log_requester_server_proxy_start[0]) == 1, f\"{int(log_requester_server_proxy_start[0])=} != 1\"\n",
    "        \n",
    "        # Ensure that the log line is present in the network-requester log that the proxied HTTP request concluded.\n",
    "        log_requester_server_proxy_end = ! grep -c \"> Proxy for 127.0.0.1:9909 is finished  (currently there are 0 proxies being handled)$\" {log_requester_server}\n",
    "        assert int(log_requester_server_proxy_end[0]) == 1, f\"{int(log_requester_server_proxy_end[0])=} != 1\"\n",
    "        \n",
    "        # Ensure that the log line is present in the socks5-client log that the proxied HTTP request concluded.\n",
    "        log_socks5_client_proxy_end = ! grep -c \"> Proxy for 127.0.0.1:9909 is finished (id: \" {log_socks5_client}\n",
    "        assert int(log_socks5_client_proxy_end[0]) == 1, f\"{int(log_socks5_client_proxy_end[0])=} != 1\"\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\")\n",
    "    for run_succeeded_file in runs_succeeded_files:\n",
    "        \n",
    "        # Obtain the respective requester-server log file of this SUCCEEDED run.\n",
    "        log_requester_server = glob(join(dirname(run_succeeded_file), \"logs_requester-server_run-*.log\"))[0]\n",
    "        \n",
    "        # Look for log line listing document.txt before runID was appended to it.\n",
    "        log_requester_server_doc_size_initial = ! grep \"^-rw-r--r-- 1 root root 1048566 \" {log_requester_server} | grep -c \"/webserver_directory/document.txt\"\n",
    "        assert int(log_requester_server_doc_size_initial[0]) == 1, f\"{int(log_requester_server_doc_size_initial[0])=} != 1\"\n",
    "        \n",
    "        # Look for log line listing document.txt after runID was appended to it.\n",
    "        log_requester_server_doc_size_final = ! grep \"^-rw-r--r-- 1 root root 1048576 \" {log_requester_server} | grep -c \"/webserver_directory/document.txt\"\n",
    "        assert int(log_requester_server_doc_size_final[0]) == 1, f\"{int(log_requester_server_doc_size_final[0])=} != 1\"\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\")\n",
    "    webserver_doc_hashes = np.zeros(len(runs_succeeded), dtype = (np.unicode_, 128))\n",
    "    \n",
    "    for idx, run_succeeded_file in enumerate(runs_succeeded_files):\n",
    "        \n",
    "        with open(run_succeeded_file, mode = \"r\", encoding = \"utf-8\") as run_succeeded_fp:\n",
    "            \n",
    "            # Extract both documents' SHA512 hash from file (first part of each line).\n",
    "            curl_doc_hash = str(run_succeeded_fp.readline().split(\" \")[0])\n",
    "            webserver_doc_hash = str(run_succeeded_fp.readline().split(\" \")[0])\n",
    "            assert curl_doc_hash == webserver_doc_hash, f\"{curl_doc_hash=} != {webserver_doc_hash=}\"\n",
    "            \n",
    "            # Store hash of webserver's document for later check.\n",
    "            webserver_doc_hashes[idx] = webserver_doc_hash\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that all found webserver documents are unique...\")\n",
    "    webserver_doc_hashes_unique = np.unique(webserver_doc_hashes)\n",
    "    print(f\"{len(webserver_doc_hashes)=}\")\n",
    "    print(f\"{len(webserver_doc_hashes_unique)=}\")\n",
    "    assert len(webserver_doc_hashes_unique) == len(webserver_doc_hashes), \\\n",
    "        f\"{len(webserver_doc_hashes_unique)=} != {len(webserver_doc_hashes)=}\"\n",
    "    assert webserver_doc_hashes_unique.size == webserver_doc_hashes.size, \\\n",
    "        f\"{webserver_doc_hashes_unique.size=} != {webserver_doc_hashes.size=}\"\n",
    "    assert webserver_doc_hashes_unique.shape == webserver_doc_hashes.shape, \\\n",
    "        f\"{webserver_doc_hashes_unique.shape=} != {webserver_doc_hashes.shape=}\"\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\")\n",
    "    \n",
    "    print(f\"{len(networkreq_confs)=}\")\n",
    "    print(f\"{len(socks5client_confs)=}\")\n",
    "    assert len(networkreq_confs) == len(socks5client_confs), f\"{len(networkreq_confs)=} != {len(socks5client_confs)=}\"\n",
    "    \n",
    "    # Ensure that all SUCCEEDED run nym-network-requester configuration files\n",
    "    # contain the expected part regarding timing and delay values.\n",
    "    for networkreq_conf in networkreq_confs:\n",
    "        \n",
    "        with open(networkreq_conf, mode = \"r\", encoding = \"utf-8\") as conf:\n",
    "            \n",
    "            conf_data = conf.read()\n",
    "            assert \"average_packet_delay = '50ms'\\naverage_ack_delay = '50ms'\\nloop_cover_traffic_average_delay = '200ms'\\nmessage_sending_average_delay = '20ms'\\n\" in conf_data\n",
    "    \n",
    "    # Ensure that all SUCCEEDED run nym-socks5-client configuration files\n",
    "    # contain the expected part regarding timing and delay values.\n",
    "    for socks5client_conf in socks5client_confs:\n",
    "        \n",
    "        with open(socks5client_conf, mode = \"r\", encoding = \"utf-8\") as conf:\n",
    "            \n",
    "            conf_data = conf.read()\n",
    "            assert \"average_packet_delay = '50ms'\\naverage_ack_delay = '50ms'\\nloop_cover_traffic_average_delay = '200ms'\\nmessage_sending_average_delay = '20ms'\\n\" in conf_data\n",
    "    \n",
    "    \n",
    "    print(\"\\n->-> Check that FAILED runs fail for the single known failure reason...\")\n",
    "    for idx, run_failed_file in enumerate(runs_failed_files):\n",
    "        \n",
    "        with open(run_failed_file, mode = \"r\", encoding = \"utf-8\") as run_failed_fp:\n",
    "            \n",
    "            err_msg = str(run_failed_fp.read().strip())\n",
    "            assert err_msg == \"Proxy between curl=>socks5-client and network-requester=>webserver did not complete\", \\\n",
    "                f\"{err_msg=} != 'Proxy between curl=>socks5-client and network-requester=>webserver did not complete'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138029b9-27f5-40fc-a2e5-b683b14ebe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raw_endpoints_results(path: str):\n",
    "    \n",
    "    # Define all paths relevant to endpoints results.\n",
    "    script_endp = join(path, \"5-exp08-run-experiments-nym-endpoints.sh\")\n",
    "    log_endp = join(path, \"logs_5-exp08-run-experiments-nym-endpoints.log\")\n",
    "    \n",
    "    # Apply all checks.\n",
    "    check_endp_script(script_endp)\n",
    "    check_endp_log(path, log_endp)\n",
    "    check_runs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fba53e0-d6d2-4899-a4ce-0c8b8453a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gateway_folder = sorted(glob(join(RAW_EXP_DIR, f\"*_mixcorr-nym-gateway-ccx22-exp08_{EXP_NAME}\")))[0]\n",
    "raw_endpoints_folders = sorted(glob(join(RAW_EXP_DIR, f\"*_mixcorr-nym-endpoints-ccx22-exp08-*_{EXP_NAME}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047a13c2-7959-4979-be5f-f15a4cf39420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- [2023-04-24_15:33:10.322822] Begin merging previously `split` files again -----\n",
      "\n",
      "-> Going to merge again the following previously `split` files:\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/gateway_sphinxflows/9Pff2ZEXMUozH6DwcwJHzn8sojymc1NmYsdak2NpuMtF_gateway-to-endpoint.sphinxflow.01\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/gateway_sphinxflows/9Pff2ZEXMUozH6DwcwJHzn8sojymc1NmYsdak2NpuMtF_gateway-to-endpoint.sphinxflow.02\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.01\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.02\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.03\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.04\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.05\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.06\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.07\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.08\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.09\n",
      "/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/logs_4-exp08-run-experiments-nym-gateway.log.10\n",
      "\n",
      "----- [2023-04-24_15:33:13.587541] End merging previously `split` files again -----\n",
      "\n",
      "\n",
      "\n",
      "----- [2023-04-24_15:33:13.587630] Begin running assertion checks on raw exp08 dataset -----\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking the gateway's experiment-running script '4-exp08-run-experiments-nym-gateway.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "\n",
      "-> Checking the gateway's experiment-running log file 'logs_4-exp08-run-experiments-nym-gateway.log'...\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that no log line contains the message that stored messages were pushed to an endpoint...\n",
      "pushed_stored_msgs_not_present=[]\n",
      "\n",
      "\n",
      "-> Checking SphinxFlow logs in '/PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download/gateway_sphinxflows'...\n",
      "\n",
      "->-> Check that timestamps are monotonically increasing per SphinxFlow log file and observed message sizes are restricted to be in { 53, 1675, 2413 }...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-21-52_mixcorr-nym-gateway-ccx22-exp08_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-23-39_mixcorr-nym-endpoints-ccx22-exp08-one_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking experiment-running script of one endpoints instance '5-exp08-run-experiments-nym-endpoints.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\n",
      "     script_correct_file_size[0]='1'\n",
      "\n",
      "->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\n",
      "     script_correct_gw_data[0]='1'\n",
      "\n",
      "\n",
      "-> Checking experiment log file 'logs_5-exp08-run-experiments-nym-endpoints.log'...\n",
      "\n",
      "->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\n",
      "     log_correct_gw_identity_key[0]='1'\n",
      "\n",
      "->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\n",
      "     log_correct_gw_sphinx_key[0]='1'\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Checking that random file of correct size has been created...\n",
      "\n",
      "\n",
      "-> Checking result folders of all runs in this experiment...\n",
      "\n",
      "->-> Of a total of len(runs_dirs)=6724 runs, len(runs_succeeded_files)=6723 SUCCEEDED and len(runs_failed_files)=0 FAILED...\n",
      "\n",
      "->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\n",
      "len(runs_completed)=6723\n",
      "len(runs_completed_unique)=6723\n",
      "\n",
      "->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \"Proxy (started|finished)\" log lines)...\n",
      "\n",
      "->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\n",
      "\n",
      "->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\n",
      "\n",
      "->-> Check that all found webserver documents are unique...\n",
      "len(webserver_doc_hashes)=6723\n",
      "len(webserver_doc_hashes_unique)=6723\n",
      "\n",
      "->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\n",
      "len(networkreq_confs)=6723\n",
      "len(socks5client_confs)=6723\n",
      "\n",
      "->-> Check that FAILED runs fail for the single known failure reason...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-23-39_mixcorr-nym-endpoints-ccx22-exp08-one_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-24-24_mixcorr-nym-endpoints-ccx22-exp08-two_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking experiment-running script of one endpoints instance '5-exp08-run-experiments-nym-endpoints.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\n",
      "     script_correct_file_size[0]='1'\n",
      "\n",
      "->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\n",
      "     script_correct_gw_data[0]='1'\n",
      "\n",
      "\n",
      "-> Checking experiment log file 'logs_5-exp08-run-experiments-nym-endpoints.log'...\n",
      "\n",
      "->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\n",
      "     log_correct_gw_identity_key[0]='1'\n",
      "\n",
      "->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\n",
      "     log_correct_gw_sphinx_key[0]='1'\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Checking that random file of correct size has been created...\n",
      "\n",
      "\n",
      "-> Checking result folders of all runs in this experiment...\n",
      "\n",
      "->-> Of a total of len(runs_dirs)=7159 runs, len(runs_succeeded_files)=7158 SUCCEEDED and len(runs_failed_files)=0 FAILED...\n",
      "\n",
      "->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\n",
      "len(runs_completed)=7158\n",
      "len(runs_completed_unique)=7158\n",
      "\n",
      "->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \"Proxy (started|finished)\" log lines)...\n",
      "\n",
      "->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\n",
      "\n",
      "->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\n",
      "\n",
      "->-> Check that all found webserver documents are unique...\n",
      "len(webserver_doc_hashes)=7158\n",
      "len(webserver_doc_hashes_unique)=7158\n",
      "\n",
      "->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\n",
      "len(networkreq_confs)=7158\n",
      "len(socks5client_confs)=7158\n",
      "\n",
      "->-> Check that FAILED runs fail for the single known failure reason...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-24-24_mixcorr-nym-endpoints-ccx22-exp08-two_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-24-43_mixcorr-nym-endpoints-ccx22-exp08-three_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking experiment-running script of one endpoints instance '5-exp08-run-experiments-nym-endpoints.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\n",
      "     script_correct_file_size[0]='1'\n",
      "\n",
      "->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\n",
      "     script_correct_gw_data[0]='1'\n",
      "\n",
      "\n",
      "-> Checking experiment log file 'logs_5-exp08-run-experiments-nym-endpoints.log'...\n",
      "\n",
      "->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\n",
      "     log_correct_gw_identity_key[0]='1'\n",
      "\n",
      "->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\n",
      "     log_correct_gw_sphinx_key[0]='1'\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Checking that random file of correct size has been created...\n",
      "\n",
      "\n",
      "-> Checking result folders of all runs in this experiment...\n",
      "\n",
      "->-> Of a total of len(runs_dirs)=6866 runs, len(runs_succeeded_files)=6865 SUCCEEDED and len(runs_failed_files)=0 FAILED...\n",
      "\n",
      "->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\n",
      "len(runs_completed)=6865\n",
      "len(runs_completed_unique)=6865\n",
      "\n",
      "->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \"Proxy (started|finished)\" log lines)...\n",
      "\n",
      "->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\n",
      "\n",
      "->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\n",
      "\n",
      "->-> Check that all found webserver documents are unique...\n",
      "len(webserver_doc_hashes)=6865\n",
      "len(webserver_doc_hashes_unique)=6865\n",
      "\n",
      "->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\n",
      "len(networkreq_confs)=6865\n",
      "len(socks5client_confs)=6865\n",
      "\n",
      "->-> Check that FAILED runs fail for the single known failure reason...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-24-43_mixcorr-nym-endpoints-ccx22-exp08-three_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-25-03_mixcorr-nym-endpoints-ccx22-exp08-four_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking experiment-running script of one endpoints instance '5-exp08-run-experiments-nym-endpoints.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\n",
      "     script_correct_file_size[0]='1'\n",
      "\n",
      "->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\n",
      "     script_correct_gw_data[0]='1'\n",
      "\n",
      "\n",
      "-> Checking experiment log file 'logs_5-exp08-run-experiments-nym-endpoints.log'...\n",
      "\n",
      "->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\n",
      "     log_correct_gw_identity_key[0]='1'\n",
      "\n",
      "->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\n",
      "     log_correct_gw_sphinx_key[0]='1'\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Checking that random file of correct size has been created...\n",
      "\n",
      "\n",
      "-> Checking result folders of all runs in this experiment...\n",
      "\n",
      "->-> Of a total of len(runs_dirs)=8146 runs, len(runs_succeeded_files)=8146 SUCCEEDED and len(runs_failed_files)=0 FAILED...\n",
      "\n",
      "->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\n",
      "len(runs_completed)=8146\n",
      "len(runs_completed_unique)=8146\n",
      "\n",
      "->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \"Proxy (started|finished)\" log lines)...\n",
      "\n",
      "->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\n",
      "\n",
      "->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\n",
      "\n",
      "->-> Check that all found webserver documents are unique...\n",
      "len(webserver_doc_hashes)=8146\n",
      "len(webserver_doc_hashes_unique)=8146\n",
      "\n",
      "->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\n",
      "len(networkreq_confs)=8146\n",
      "len(socks5client_confs)=8146\n",
      "\n",
      "->-> Check that FAILED runs fail for the single known failure reason...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-25-03_mixcorr-nym-endpoints-ccx22-exp08-four_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- BEGIN /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-25-31_mixcorr-nym-endpoints-ccx22-exp08-five_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "-> Checking experiment-running script of one endpoints instance '5-exp08-run-experiments-nym-endpoints.sh'...\n",
      "\n",
      "->-> script_correct_exp_id[0]='1'  =>  Checking that we see the expected ID 'exp08' of the experiment to conduct in the script...\n",
      "     script_correct_exp_id[0]='1'\n",
      "\n",
      "->-> script_correct_nym_version[0]='1'  =>  Checking that we see the expected Nym git tag specified...\n",
      "     script_correct_nym_version[0]='1'\n",
      "\n",
      "->-> script_correct_exp_name[0]='1'  =>  Checking that we see the expected experiment name specified...\n",
      "     script_correct_exp_name[0]='1'\n",
      "\n",
      "->-> script_correct_file_size[0]='1'  =>  Checking that we see the correct file size to be generated specified...\n",
      "     script_correct_file_size[0]='1'\n",
      "\n",
      "->-> script_correct_gw_data[0]='1'  =>  Checking that we see the correct hard-coded gateway data...\n",
      "     script_correct_gw_data[0]='1'\n",
      "\n",
      "\n",
      "-> Checking experiment log file 'logs_5-exp08-run-experiments-nym-endpoints.log'...\n",
      "\n",
      "->-> log_correct_gw_identity_key[0]='1'  =>  Checking that we see the correct gateway identity key in the log...\n",
      "     log_correct_gw_identity_key[0]='1'\n",
      "\n",
      "->-> log_correct_gw_sphinx_key[0]='1'  =>  Checking that we see the correct gateway sphinx key in the log...\n",
      "     log_correct_gw_sphinx_key[0]='1'\n",
      "\n",
      "->-> Check that orchestration repository is checked out at the expected tag...\n",
      "\n",
      "->-> Check that experiment scenarios repository is checked out at the expected tag...\n",
      "\n",
      "->-> Checking that random file of correct size has been created...\n",
      "\n",
      "\n",
      "-> Checking result folders of all runs in this experiment...\n",
      "\n",
      "->-> Of a total of len(runs_dirs)=6720 runs, len(runs_succeeded_files)=6719 SUCCEEDED and len(runs_failed_files)=0 FAILED...\n",
      "\n",
      "->-> Check that only a single result file (SUCCEEDED, FAILED) exists in each run directory...\n",
      "len(runs_completed)=6719\n",
      "len(runs_completed_unique)=6719\n",
      "\n",
      "->-> Check that each SUCCEEDED run actually used the SOCKS5 client to download the file (by way of checking for \"Proxy (started|finished)\" log lines)...\n",
      "\n",
      "->-> Check that the requester-server log of each SUCCEEDED run shows the correct two document.txt sizes...\n",
      "\n",
      "->-> Check that the SHA512 hashes of the curl client and webserver documents are equal...\n",
      "\n",
      "->-> Check that all found webserver documents are unique...\n",
      "len(webserver_doc_hashes)=6719\n",
      "len(webserver_doc_hashes_unique)=6719\n",
      "\n",
      "->-> Checking that all SUCCEEDED client configuration values related to timing/delay are what we expect for exp08...\n",
      "len(networkreq_confs)=6719\n",
      "len(socks5client_confs)=6719\n",
      "\n",
      "->-> Check that FAILED runs fail for the single known failure reason...\n",
      "\n",
      "\n",
      "-----  END  /PRIVATE_PATH/dataset_exp08_nym-binaries-v1.1.13_static-http-download_raw/2023-04-14_16-25-31_mixcorr-nym-endpoints-ccx22-exp08-five_exp08_nym-binaries-v1.1.13_static-http-download -----\n",
      "\n",
      "\n",
      "\n",
      "----- [2023-04-24_15:47:12.468387] Done running assertion checks on raw exp08 dataset -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Begin merging previously `split` files again -----\\n\")\n",
    "\n",
    "# Find all files ending in '.[0-9][0-9]', indicating they are part of a larger file that\n",
    "# was previously `split` into smaller ones in order to be able to upload to GitHub.\n",
    "prev_split_files = ! find {RAW_EXP_DIR} -type f -name \"*.[0-9][0-9]\" | sort -d\n",
    "\n",
    "if len(prev_split_files) > 0:\n",
    "    \n",
    "    print(\"-> Going to merge again the following previously `split` files:\")\n",
    "    print(*prev_split_files, sep = \"\\n\")\n",
    "    print()\n",
    "    \n",
    "    prev_split_files_bases = set()\n",
    "    \n",
    "    for prev_split_file in prev_split_files:\n",
    "        prev_split_files_bases.add(prev_split_file.rsplit(\".\", 1)[0])\n",
    "    \n",
    "    for prev_split_files_base in prev_split_files_bases:\n",
    "        ! cat {prev_split_files_base}.?? > {prev_split_files_base}\n",
    "\n",
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] End merging previously `split` files again -----\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Begin running assertion checks on raw exp08 dataset -----\\n\")\n",
    "\n",
    "\n",
    "# Start with checking the raw gateway result files.\n",
    "print(f\"\\n\\n----- BEGIN {raw_gateway_folder} -----\\n\")\n",
    "check_raw_gateway_results(raw_gateway_folder)\n",
    "print(f\"\\n\\n-----  END  {raw_gateway_folder} -----\\n\\n\\n\")\n",
    "\n",
    "\n",
    "for raw_endpoints_folder in raw_endpoints_folders:\n",
    "    \n",
    "    # Then, check each endpoints-related raw results folder.\n",
    "    print(f\"\\n\\n----- BEGIN {raw_endpoints_folder} -----\\n\")\n",
    "    check_raw_endpoints_results(raw_endpoints_folder)\n",
    "    print(f\"\\n\\n-----  END  {raw_endpoints_folder} -----\\n\\n\\n\")\n",
    "\n",
    "    \n",
    "print(f\"----- [{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}] Done running assertion checks on raw exp08 dataset -----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
